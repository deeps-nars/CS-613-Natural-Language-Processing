{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 of S Deepak Narayanan, 16110142"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. follows below \n",
    "### Question 1. is just to download the dataset, which has already been done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization using Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "file = open('alice.txt').read()\n",
    "data_temp = sent_tokenize(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Dataset cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_temp = data\n",
    "for dt in range(len(data_temp)):\n",
    "    data_temp[dt] = data_temp[dt].replace('\\n', \"\")\n",
    "    data_temp[dt] = data_temp[dt].replace('\\n', \"\")\n",
    "    data_temp[dt] = data_temp[dt].replace('\\ ',\" \")\n",
    "    data_temp[dt] = data_temp[dt].replace('\"',' ' )\n",
    "    data_temp[dt] = data_temp[dt].replace(\"  \",\" \")\n",
    "    data_temp[dt] = data_temp[dt].replace(\".\",\" \")\n",
    "data_final = []\n",
    "for data in data_temp:\n",
    "    data = '<s> '+data\n",
    "    data = data + ' </s>'\n",
    "    data_final.append(data)\n",
    "# Final Data is Data Final itself\n",
    "del data_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(data_final, test_size = 0.2, random_state= 10)\n",
    "del train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3. follows below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Model and MLE for Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_count = {}\n",
    "for data in data_final:\n",
    "    arr = data.split()\n",
    "    for elem in arr:\n",
    "        if elem in unigram_count:\n",
    "            unigram_count[elem]+=1\n",
    "        else:\n",
    "            unigram_count[elem]=1\n",
    "## Unigram Count is a dictionary that contains all the Unigrams with their number of occurrences in the corpus.\n",
    "total_count = sum(unigram_count.values())\n",
    "mle_unigram = {}\n",
    "for elem in unigram_count:\n",
    "    mle_unigram[elem] = unigram_count[elem]/total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Model and MLE for Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Count contains all the bigrams with their number of occurrences in the corpus. \n",
    "bigram_count = {}\n",
    "for data in data_final:\n",
    "    tmp_arr = data.split()\n",
    "    for i in range(len(tmp_arr)):\n",
    "        try:\n",
    "            if (tmp_arr[i],tmp_arr[i+1]) in bigram_count:\n",
    "                bigram_count[(tmp_arr[i],tmp_arr[i+1])]+=1\n",
    "            if (tmp_arr[i], tmp_arr[i+1]) not in bigram_count:\n",
    "                 bigram_count[(tmp_arr[i],tmp_arr[i+1])]=1\n",
    "        except:\n",
    "            continue\n",
    "mle_bigram = {}\n",
    "for i in bigram_count:\n",
    "    mle_bigram[i] = bigram_count[i]/unigram_count[i[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Count and MLE for Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_count = {}\n",
    "for data in data_final:\n",
    "    tmp_arr = data.split()\n",
    "    for i in range(len(tmp_arr)):\n",
    "        try:\n",
    "            if (tmp_arr[i],tmp_arr[i+1],tmp_arr[i+2]) in trigram_count:\n",
    "                trigram_count[(tmp_arr[i],tmp_arr[i+1],tmp_arr[i+2])]+=1\n",
    "            if (tmp_arr[i], tmp_arr[i+1],tmp_arr[i+2]) not in trigram_count:\n",
    "                 trigram_count[(tmp_arr[i],tmp_arr[i+1],tmp_arr[i+2])]=1\n",
    "        except:\n",
    "            continue\n",
    "mle_trigram = {}\n",
    "for i in trigram_count:\n",
    "    mle_trigram[i] = trigram_count[i]/bigram_count[(i[0],i[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadgram Count and MLE for Quadgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadgram_count = {}\n",
    "for data in data_final:\n",
    "    tmp_arr = data.split()\n",
    "    for i in range(len(tmp_arr)):\n",
    "        try:\n",
    "            if (tmp_arr[i],tmp_arr[i+1],tmp_arr[i+2], tmp_arr[i+3]) in quadgram_count:\n",
    "                quadgram_count[(tmp_arr[i],tmp_arr[i+1],tmp_arr[i+2], tmp_arr[i+3])]+=1\n",
    "            if (tmp_arr[i], tmp_arr[i+1],tmp_arr[i+2], tmp_arr[i+3]) not in quadgram_count:\n",
    "                 quadgram_count[(tmp_arr[i],tmp_arr[i+1],tmp_arr[i+2], tmp_arr[i+3])]=1\n",
    "        except:\n",
    "            continue\n",
    "mle_quadgram = {}\n",
    "for i in quadgram_count:\n",
    "    mle_quadgram[i] = quadgram_count[i]/trigram_count[(i[0],i[1], i[2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below unic, bigc, tric, quadc, contains the number of unigrams, bigrams, trigrams and quadgrams in the corpus. Using the entire corpus in this case. The number of possible ones is also covered in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of n-grams possible and those present are respectively\n",
      "1-gram 5949,5949\n",
      "2-gram 35390601,17565\n",
      "3-gram 210538685349,22750\n",
      "4-gram 1252494639141201,23287\n"
     ]
    }
   ],
   "source": [
    "unic = len(unigram_count)\n",
    "bigc = len(bigram_count)\n",
    "tric = len(trigram_count)\n",
    "quadc = len(quadgram_count)\n",
    "\n",
    "possible= {i:0 for i in range(1,5)}\n",
    "possible[1] = unic\n",
    "possible[2] = unic**2\n",
    "possible[3] = unic**3\n",
    "possible[4] = unic**4\n",
    "\n",
    "present = {1:unic, 2:bigc, 3:tric, 4:quadc}\n",
    "print(\"The total number of n-grams possible and those present are respectively\")\n",
    "for i in present:\n",
    "    print(str(i) + '-gram '+str(possible[i])+','+str(present[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. (b) Sentence Probabilities\n",
    "### I am randomly taking 10 sentences from the corpus and displaying their probability using the unigram, bigram, trigram and the quadgram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def prob_sent(sentence, ngram):\n",
    "    \n",
    "    # Probability of Formation of a sentence using a Unigram\n",
    "    if ngram==1:\n",
    "        flag = 1\n",
    "        array = sentence.split()\n",
    "        product = 0\n",
    "        for i in array:\n",
    "            try:\n",
    "                product+=mle_unigram[i]\n",
    "            except:\n",
    "                flag = 0\n",
    "                break\n",
    "        if(flag==0):\n",
    "            return 100\n",
    "        return product, math.exp(product)\n",
    "\n",
    "    # Probability of formation of a sentence using a bigram\n",
    "    if ngram==2:\n",
    "        flag = 1\n",
    "        array = sentence.split()\n",
    "        product = 0\n",
    "        for i in range(len(array)-1):\n",
    "            try:\n",
    "                product+=mle_bigram[(array[i],array[i+1])]\n",
    "            except:\n",
    "                flag = 0\n",
    "                break\n",
    "        if(flag==0):\n",
    "            return 100\n",
    "        return product, math.exp(product)\n",
    "    \n",
    "    # Probability of sentence formation using trigrams\n",
    "    if ngram==3:\n",
    "        flag = 1\n",
    "        array = sentence.split()\n",
    "        product = 1\n",
    "        for i in range(len(array)-2):\n",
    "            try:\n",
    "                product+=log(mle_trigram[(array[i],array[i+1], array[i+2])])\n",
    "            except:\n",
    "                flag = 0\n",
    "                break\n",
    "        if(flag==0):\n",
    "            return 100\n",
    "        return product, math.exp(product)\n",
    "    \n",
    "    #Probability of a sentence using quadgram \n",
    "    if ngram==4:\n",
    "        flag = 1\n",
    "        array = sentence.split()\n",
    "        product = 0\n",
    "        for i in range(len(array)-3):\n",
    "            try:\n",
    "                product+=log(mle_quadgram[(array[i],array[i+1], array[i+2], array[i+3])])\n",
    "            except:\n",
    "                flag = 0\n",
    "                break\n",
    "        if(flag==0):\n",
    "            return 100\n",
    "        return product, math.exp(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results are below: If 100, then the probability is 0, else the values are those computed from log probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two tuple below shows probability in log scale and actual probability\n",
      "The sentence chosen is \n",
      " <s> ‘I dare say you never even spoke to Time!’‘Perhaps not,’ Alice cautiously replied: ‘but I know I have to beat timewhen I learn music ’‘Ah! </s>\n",
      "The Probability using 1 gram model is  (0.178200821201941, 1.1950652915866815)\n",
      "The Probability using 2 gram model is  (6.555769420518711, 703.2900675082301)\n",
      "The Probability using 3 gram model is  (-12.888017248751021, 2.5281639029567037e-06)\n",
      "The Probability using 4 gram model is  (-1.3862943611198906, 0.25)\n",
      "\n",
      "The sentence chosen is \n",
      " <s> ‘He denies it,’ said the King: ‘leave out that part ’‘Well, at any rate, the Dormouse said--’ the Hatter went on, lookinganxiously round to see if he would deny it too: but the Dormouse deniednothing, being fast asleep  </s>\n",
      "The Probability using 1 gram model is  (0.36722657708100037, 1.4437249970551962)\n",
      "The Probability using 2 gram model is  (10.756992309815788, 46957.22264767012)\n",
      "The Probability using 3 gram model is  (-29.068548905285326, 2.375142186363859e-13)\n",
      "The Probability using 4 gram model is  (-5.817111159963205, 0.002976190476190474)\n",
      "\n",
      "The sentence chosen is \n",
      " <s> I’m never sure what I’m goingto be, from one minute to another! </s>\n",
      "The Probability using 1 gram model is  (0.10776409107876073, 1.1137849625822671)\n",
      "The Probability using 2 gram model is  (2.8635611744599134, 17.523821335113563)\n",
      "The Probability using 3 gram model is  (-1.0794415416798357, 0.3397852285573807)\n",
      "The Probability using 4 gram model is  (0.0, 1.0)\n",
      "\n",
      "The sentence chosen is \n",
      " <s> So she set to work, and very soon finished off the cake  </s>\n",
      "The Probability using 1 gram model is  (0.19451287793952965, 1.2147191258637668)\n",
      "The Probability using 2 gram model is  (3.388722122095184, 29.628067016956816)\n",
      "The Probability using 3 gram model is  (-7.371010681238157, 0.0006292319047358897)\n",
      "The Probability using 4 gram model is  (-2.4849066497880004, 0.08333333333333333)\n",
      "\n",
      "The sentence chosen is \n",
      " <s> ‘Now, I give you fair warning,’ shouted the Queen, stamping on theground as she spoke; ‘either you or your head must be off, and that inabout half no time! </s>\n",
      "The Probability using 1 gram model is  (0.2247480403135498, 1.2520072211006494)\n",
      "The Probability using 2 gram model is  (9.658518784294934, 15654.579677259213)\n",
      "The Probability using 3 gram model is  (-14.728566882148508, 4.0129585761217536e-07)\n",
      "The Probability using 4 gram model is  (0.0, 1.0)\n",
      "\n",
      "The sentence chosen is \n",
      " <s> ‘Beautiful Soup! </s>\n",
      "The Probability using 1 gram model is  (0.07331093691676, 1.0760650732582706)\n",
      "The Probability using 2 gram model is  (1.5010214504596529, 4.486269232505128)\n",
      "The Probability using 3 gram model is  (1.0, 2.718281828459045)\n",
      "The Probability using 4 gram model is  (0.0, 1.0)\n",
      "\n",
      "The sentence chosen is \n",
      " <s> Do come back again, and wewon’t talk about cats or dogs either, if you don’t like them!’ When theMouse heard this, it turned round and swam slowly back to her: itsface was quite pale (with passion, Alice thought), and it said in a lowtrembling voice, ‘Let us get to the shore, and then I’ll tell you myhistory, and you’ll understand why it is I hate cats and dogs ’It was high time to go, for the pool was getting quite crowded with thebirds and animals that had fallen into it: there were a Duck and a Dodo,a Lory and an Eaglet, and several other curious creatures  </s>\n",
      "The Probability using 1 gram model is  (0.7432624113475175, 2.102784484304278)\n",
      "The Probability using 2 gram model is  (24.855351307908286, 62307728493.326744)\n",
      "The Probability using 3 gram model is  (-53.673624359444894, 4.8959967059829705e-24)\n",
      "The Probability using 4 gram model is  (-2.772588722239781, 0.0625)\n",
      "\n",
      "The sentence chosen is \n",
      " <s> ‘But perhaps he can’t help it,’ shesaid to herself; ‘his eyes are so VERY nearly at the top of his head  </s>\n",
      "The Probability using 1 gram model is  (0.1867487868607689, 1.2053244536741075)\n",
      "The Probability using 2 gram model is  (4.970296811624457, 144.06964257013178)\n",
      "The Probability using 3 gram model is  (-11.108680299521524, 1.4981712017521202e-05)\n",
      "The Probability using 4 gram model is  (-3.8712010109078907, 0.02083333333333334)\n",
      "\n",
      "The sentence chosen is \n",
      " <s> Very soon the Rabbit noticed Alice, as she went hunting about, andcalled out to her in an angry tone, ‘Why, Mary Ann, what ARE you doingout here? </s>\n",
      "The Probability using 1 gram model is  (0.21478163493840988, 1.2395911840047695)\n",
      "The Probability using 2 gram model is  (7.465716321919916, 1747.1065908415123)\n",
      "The Probability using 3 gram model is  (-17.05380767142114, 3.9230643585347555e-08)\n",
      "The Probability using 4 gram model is  (-2.525728644308255, 0.08000000000000002)\n",
      "\n",
      "The sentence chosen is \n",
      " <s> ‘You don’t know much,’ said the Duchess; ‘and that’s a fact ’Alice did not at all like the tone of this remark, and thought it wouldbe as well to introduce some other subject of conversation  </s>\n",
      "The Probability using 1 gram model is  (0.3457633445315416, 1.413068165935768)\n",
      "The Probability using 2 gram model is  (7.716148521403117, 2244.2990436066784)\n",
      "The Probability using 3 gram model is  (-27.919008212629556, 7.497712666994546e-13)\n",
      "The Probability using 4 gram model is  (-3.178053830347946, 0.041666666666666644)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "print( 'The two tuple below shows probability in log scale and actual probability')\n",
    "sentence_list_test = []\n",
    "for i in range(10):\n",
    "    sentence_list_test.append(random.choice(data_final))\n",
    "for i in range(10):\n",
    "    print('The sentence chosen is \\n',sentence_list_test[i])\n",
    "    for j in range(4):\n",
    "         print('The Probability using '+str(j+1)+' gram model is ', prob_sent(sentence_list_test[i],j+1))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. (a) Generating Sentences by using MLEs for the N-Gram Model\n",
    "\n",
    "### The input to this is an integer, that can take values from 1 till 4. \n",
    "### Output would be a sentence generated\n",
    "### I have generated 10 sample sentences per n-gram below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def generate(ngram):\n",
    "    deep = -100\n",
    "    sentence = ['<s>']\n",
    "    start = '<s>'\n",
    "    if ngram==1:\n",
    "        while(start!='</s>'):\n",
    "            relevant=[]\n",
    "            for i in unigram_count:\n",
    "                relevant.append(i)\n",
    "            prob = []\n",
    "            for i in relevant:\n",
    "                prob.append(mle_unigram[i])\n",
    "            arr = np.random.multinomial(1, prob,size = None)\n",
    "            max_ = {0:0}\n",
    "            for i in arr:\n",
    "                max_[0] = max(max_[0],i)\n",
    "            for i in range(len(arr)):\n",
    "                if arr[i] == max_[0]:\n",
    "                    deep = i\n",
    "                    break\n",
    "            start = relevant[deep]\n",
    "            sentence.append(start)\n",
    "            relevant=[]\n",
    "            deep = -100\n",
    "        return ' '.join(sentence)\n",
    "    if ngram==2:\n",
    "        while(start!='</s>'):\n",
    "            relevant = []\n",
    "            for i in bigram_count:\n",
    "                if i[0]==start:\n",
    "                    relevant.append(i)\n",
    "            prob = []\n",
    "            for i in relevant:\n",
    "                prob.append(mle_bigram[i])\n",
    "            arr = np.random.multinomial(1, prob,size = None)\n",
    "            max_ = {0:0}\n",
    "            for i in arr:\n",
    "                max_[0] = max(max_[0],i)\n",
    "            for i in range(len(arr)):\n",
    "                if arr[i] == max_[0]:\n",
    "                    deep = i\n",
    "                    break\n",
    "            start = relevant[deep][1]\n",
    "            sentence.append(start)\n",
    "            relevant=[]\n",
    "            deep = -100\n",
    "        return ' '.join(sentence)\n",
    "    if ngram==3:\n",
    "        start = '<s>'\n",
    "        relevant = []\n",
    "        for i in bigram_count:\n",
    "                if i[0]==start:\n",
    "                    relevant.append(i)\n",
    "        prob = []\n",
    "        for i in relevant:\n",
    "            prob.append(mle_bigram[i])\n",
    "        arr = np.random.multinomial(1, prob,size = None)\n",
    "        max_ = {0:0}\n",
    "        for i in arr:\n",
    "            max_[0] = max(max_[0],i)\n",
    "        for i in range(len(arr)):\n",
    "            if arr[i] == max_[0]:\n",
    "                deep = i\n",
    "                break\n",
    "        bigram_2 = relevant[deep][1]\n",
    "        start_trigram = ('<s>',bigram_2)\n",
    "        sentence = ['<s>',bigram_2]\n",
    "        while(1):\n",
    "            relevant = []\n",
    "            for i in trigram_count:\n",
    "                if i[0]==start and i[1] ==bigram_2:\n",
    "                    relevant.append(i)\n",
    "            prob = []\n",
    "            for i in relevant:\n",
    "                prob.append(mle_trigram[i])\n",
    "            arr = np.random.multinomial(1, prob,size = None)\n",
    "            max_ = {0:0}\n",
    "            for i in arr:\n",
    "                max_[0] = max(max_[0],i)\n",
    "            for i in range(len(arr)):\n",
    "                if arr[i] == max_[0]:\n",
    "                    deep = i\n",
    "                    break\n",
    "            start = relevant[deep][1]\n",
    "            bigram_2=relevant[deep][2]\n",
    "            sentence.append(bigram_2)\n",
    "            relevant=[]\n",
    "            deep = -100\n",
    "            if bigram_2=='</s>':\n",
    "                break\n",
    "        return ' '.join(sentence)\n",
    "    if ngram==4:\n",
    "        start = '<s>'\n",
    "        relevant = []\n",
    "        for i in bigram_count:\n",
    "                if i[0]==start:\n",
    "                    relevant.append(i)\n",
    "        prob = []\n",
    "        for i in relevant:\n",
    "            prob.append(mle_bigram[i])\n",
    "        arr = np.random.multinomial(1, prob,size = None)\n",
    "        max_ = {0:0}\n",
    "        for i in arr:\n",
    "            max_[0] = max(max_[0],i)\n",
    "        for i in range(len(arr)):\n",
    "            if arr[i] == max_[0]:\n",
    "                deep = i\n",
    "                break\n",
    "        bigram_2 = relevant[deep][1]\n",
    "        sentence = ['<s>',bigram_2]\n",
    "        relevant = []\n",
    "        for i in trigram_count:\n",
    "            if i[0]==start and i[1] ==bigram_2:\n",
    "                relevant.append(i)\n",
    "        prob = []\n",
    "        for i in relevant:\n",
    "            prob.append(mle_trigram[i])\n",
    "        arr = np.random.multinomial(1, prob,size = None)\n",
    "        max_ = {0:0}\n",
    "        for i in arr:\n",
    "            max_[0] = max(max_[0],i)\n",
    "        for i in range(len(arr)):\n",
    "            if arr[i] == max_[0]:\n",
    "                deep = i\n",
    "                break\n",
    "        bigram_2 = relevant[deep][1]\n",
    "        trigram_3=relevant[deep][2]\n",
    "        sentence.append(trigram_3)\n",
    "        while(1):\n",
    "            relevant = []\n",
    "            for i in quadgram_count:\n",
    "                if i[0]==start and i[1] ==bigram_2 and i[2]==trigram_3:\n",
    "                    relevant.append(i)\n",
    "            prob = []\n",
    "            for i in relevant:\n",
    "                prob.append(mle_quadgram[i])\n",
    "            arr = np.random.multinomial(1, prob,size = None)\n",
    "            max_ = {0:0}\n",
    "            for i in arr:\n",
    "                max_[0] = max(max_[0],i)\n",
    "            for i in range(len(arr)):\n",
    "                if arr[i] == max_[0]:\n",
    "                    deep = i\n",
    "                    break\n",
    "            #print(relevant)\n",
    "            start = relevant[deep][1]\n",
    "            bigram_2=relevant[deep][2]\n",
    "            trigram_3 = relevant[deep][3]\n",
    "            #print(trigram_3)\n",
    "            sentence.append(trigram_3)\n",
    "            relevant=[]\n",
    "            deep = -100\n",
    "            if trigram_3=='</s>':\n",
    "                break\n",
    "        return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is the sentence generated using unigram \n",
      "\n",
      "\n",
      "<s> felt ’‘But silence:at a Oh, and changed, and Alice, now stupidly into send humbly: fact here? ‘to world a <s> said said </s>\n",
      "<s> <s> succeeded you, a long flustered song, Oh, is!’‘Why me! again of Quadrille she had ’‘Nobody IS putthe Alice; they <s> tea,’ than talking the would a round, ‘and <s> one sneezing, the out passion, thought end </s>\n",
      "<s> of bythe ‘or that (We youth, Alice perfectlyround, and room theothers threwthemselves could yourfinger the she bebeheaded, the out upon ’‘I back she bowed, and to </s>\n",
      "<s> end! ‘I they vanished began everymoment can’t you’ll with the had the ‘But the exclaimed the THE The stuff? </s>\n",
      "<s> saying said wenton theLory Cheshire over refused </s>\n",
      "<s> asleep white all under moment at nurse Lory VIII <s> pace,’ far inher the could, OF little her, you said the depends she the remark checked dreadfullypuzzled it story,’ some good said first to pig, </s>\n",
      "<s> say, all her, sleepy, a can to </s>\n",
      "<s> </s>\n",
      "<s> quietthing,’ is--“Birds it all door; a but on tired days all to the him </s>\n",
      "<s> Alice didn’t often, it told she VOICE than she on!’ ‘Wake be,’ I ’‘I was am play very but the last are liked things best quiteunderstand </s>\n",
      "\n",
      " This is the sentence generated using bigram \n",
      "\n",
      "\n",
      "<s> Please, Ma’am, is thirteen, andfour times as curious thing before, ‘and I look for a wretched Hatter was Bill, wasin the rats and--oh dear, how to cry again as the door; so it just at all crowdedround it, andbehind it, and burning with you, won’t talk about it, and then, ‘we learned French mouse, you know </s>\n",
      "<s> ‘Well, I’ll stay down here! </s>\n",
      "<s> ‘We had at thesudden change, but the Caterpillar took the cook </s>\n",
      "<s> ‘Then it must have finished,’ said Alice did the look at one repeat it,but her chin: it won’t you, won’t you know </s>\n",
      "<s> ‘I’ve often readin the lefthand bit hurt, and marked, with the Cat sitting on again: ‘Twenty-four hours, I beat him thecrown </s>\n",
      "<s> But thereseemed to Alice, ‘shall I can have got no time they were trying toinvent something!’‘I--I’m a narrow escape!’ said Alice was trickling down to the words: ‘Where’s the nextwitness ’ said the Queen’s hedgehog had to put her face--and she saw Alice </s>\n",
      "<s> For the White Rabbit came in the race is it?’The Gryphon saidto the conversation </s>\n",
      "<s> ‘That’s right!’ shouted Alice waited patiently until there were live at last, they both mad ’‘How are all alone here!’As she thought, and have got into thecourt, arm-in-arm with a moment’s delay wouldcost them up, and all very truthfulchild; ‘but then--Ishouldn’t be a large mushroom in her </s>\n",
      "<s> ‘It’s reallydreadful,’ she wouldkeep, through next day, maybe,’ the other side will you, won’t she could not help it,’ said the Hatter, ‘or I’ll take theroof off together, Alice could speak again very solemnly dancing ’ As she would be two feet high time they were in a few minutes she had not join the well go through,’ thought this minute!’ She felt dreadfully ugly child: but thought Alice began bowing to herself,as well say “With what I shan’t! </s>\n",
      "<s> But she were all to eat what anignorant little </s>\n",
      "\n",
      " This is the sentence generated using trigram \n",
      "\n",
      "\n",
      "<s> ‘I make you a presentof everything I’ve said as yet ’‘A cheap sort of a dance is it?’‘Why,’ said the Dodo managed it ) </s>\n",
      "<s> Alice said very politely, feeling quitepleased to have nosort of meaning in them, after all </s>\n",
      "<s> ‘I’m afraid I can’t remember,’ said the Queen, but she did so, and giving it a bit, if you cut yourfinger VERY deeply with a trumpet in one hand,and a scroll of parchment in the last few minutes, and she at once </s>\n",
      "<s> At last the Gryphon </s>\n",
      "<s> Would not, could not, would not give all else for two Pennyworth only of beautiful Soup? </s>\n",
      "<s> ‘I shall sit here,’ he said, ‘on and off, fordays and days ’‘But what am I to do that,’ said the youth, ‘as I mentioned before, And have grown most uncommonly fat; Yet you turned a back-somersault in at thewindow, and some of the house, “Let us both go to law: I will prosecute YOU --Come, I’ll take no denial; We must have been aholiday?’‘Of course it was,’ said the Queen, and Alice, were in custody andunder sentence of execution ’‘What for?’ said Alice </s>\n",
      "<s> ‘Do youtake me for asking! </s>\n",
      "<s> Alice was not a moment to be managed? </s>\n",
      "<s> ‘It tells the day and night! </s>\n",
      "<s> ‘I’m a poor man,’ the Hatter with a melancholy way, being quite unable to move </s>\n",
      "\n",
      " This is the sentence generated using quadgram \n",
      "\n",
      "\n",
      "<s> Why,she’ll eat a little bird as soon as look at it!’This speech caused a remarkable sensation among the party </s>\n",
      "<s> How she longed to get out again </s>\n",
      "<s> ‘Veryuncomfortable for the Dormouse,’ thought Alice; ‘only, as it’s asleep, Isuppose it doesn’t mind ’The table was a large pool all round her, about four inchesdeep and reaching half down the hall </s>\n",
      "<s> ‘The trial cannot proceed,’ said the King </s>\n",
      "<s> There are nomice in the air, I’m afraid, but you might catch a bat, and that’s verylike a mouse, you know </s>\n",
      "<s> ‘There’s no sort of chance of her ever getting out of the way to hear the Rabbit say to itself, ‘Oh dear! </s>\n",
      "<s> ‘It’s no usespeaking to it,’ she thought, ‘till its ears have come, or at least oneof them ’ In another minute the whole head appeared, and then nodded </s>\n",
      "<s> ‘Oh, hush!’ the Rabbit whispered in a frightenedtone </s>\n",
      "<s> it was too slippery;and when she had tired herself out with trying, the poor little thing was waving itstail about in a melancholy way, being quite unable to move </s>\n",
      "<s> ‘They couldn’t have done that, you know,’ Alice gently remarked; ‘they’dhave been ill ’‘So they were,’ said the Dormouse; ‘VERY ill ’Alice tried to fancy what the flame of acandle is like after the candle is blown out, for she could not remember ever having heardof such a rule at processions; ‘and besides, what would be the use ofa procession,’ thought she, ‘if people had all to lie down on her facelike the three gardeners, but she could not help bursting outlaughing: and when she hadfinished, her sister kissed her, and said, ‘It WAS a curious dream,dear, certainly: but now run in to your tea; it’s getting late ’ SoAlice got up and went to the table for it, she found she had forgotten the little golden key, and Alice’sfirst thought was that it might happen any minute,‘and then,’ thought she, ‘what would become of me? </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' This is the sentence generated using unigram \\n\\n')\n",
    "for i in range(10):\n",
    "    print(generate(1))\n",
    "print()\n",
    "print(' This is the sentence generated using bigram \\n\\n')\n",
    "for i in range(10):\n",
    "    print(generate(2))\n",
    "print()\n",
    "print(' This is the sentence generated using trigram \\n\\n')\n",
    "for i in range(10):\n",
    "    print(generate(3))\n",
    "print()\n",
    "print(' This is the sentence generated using quadgram \\n\\n')\n",
    "for i in range(10):\n",
    "    print(generate(4))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5., Q6. and Q7. follow below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Add One Smoothing and Good Turing Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below cell finds all the possible bigrams and puts all of them in the bigram_total dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_total = {}\n",
    "for i in unigram_count:\n",
    "    for j in unigram_count:\n",
    "        bigram_total[(i,j)] = 0\n",
    "## Created above all the possible bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below is used for performing Add-One Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_one = {}\n",
    "for data in bigram_total:\n",
    "    if data not in bigram_count:\n",
    "        add_one[data] = (1)/(unigram_count[data[0]]+ len(unigram_count))\n",
    "    else:\n",
    "        add_one[data] = (bigram_count[data]+1)/((unigram_count[data[0]]) + len(unigram_count))\n",
    "del bigram_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drasatic Changes in Counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.000502186074736095\n",
      "1 0.0005021016878977928\n",
      "1 0.0004972553078182335\n"
     ]
    }
   ],
   "source": [
    "## The drastic changes after Add One Smoothing are\n",
    "## Example 1\n",
    "#print(mle_bigram[('pink', 'eyes')], add_one[('pink', 'eyes')])\n",
    "print(bigram_count[('pink', 'eyes')], add_one[('pink', 'eyes')]*(bigram_count[('pink', 'eyes')]+1)*len(bigram_count)/(len(unigram_count) + len(bigram_count)))\n",
    "## Example 2unigr\n",
    "#print(mle_bigram[('book,’', 'thought')], add_one[('book,’', 'thought')])\n",
    "print(bigram_count[('book,’', 'thought')], add_one[('book,’', 'thought')]*(bigram_count[('book,’', 'thought')]+1)*len(bigram_count)/(len(unigram_count) + len(bigram_count)))\n",
    "## Example 3\n",
    "#print(mle_bigram[('or', 'conversations')], add_one[('or', 'conversations')])\n",
    "print(bigram_count[('or', 'conversations')], add_one[('or', 'conversations')]*(bigram_count[('or', 'conversations')]+1)*len(bigram_count)/(len(unigram_count) + len(bigram_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above change in counts is due to the fact that Add-One has taking too much mass from something that occurs and has assigned it to something that has never occurred. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity for Add_One Smoothing on test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The perplexity of Add One Smoothing is  1324.7182741125455\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def add_one_perplexity(data):\n",
    "    prob = 0\n",
    "    count = 0\n",
    "    for i in data:\n",
    "        arr = i.split()\n",
    "        for j in range(len(arr)):\n",
    "            count+=1\n",
    "            #print(math.log(add_one[(arr[j],arr[j+1])]))\n",
    "            try:\n",
    "               \n",
    "                prob+=math.log(add_one[(arr[j],arr[j+1])])\n",
    "                #print('b')\n",
    "            except:\n",
    "                continue\n",
    "    return math.exp(-1/(count)*prob)\n",
    "print(' The perplexity of Add One Smoothing is ',add_one_perplexity(test_data))\n",
    "del add_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Turing Count and discounting value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Computed the count of the number of bigrams with a particular count\n",
    "good_turing = {}\n",
    "for i in bigram_count:\n",
    "    if bigram_count[i] in good_turing:\n",
    "        good_turing[bigram_count[i]]+=1\n",
    "    else:\n",
    "        good_turing[bigram_count[i]]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The below plot is almost a straight line for value <5. The discounting is ~0.6 to 0.8 for c<7.  \n",
    "Note that 0 has a very high count because of the fact that number of times one word occurs is the maximum. I have not taken till 10 because of missing N-c values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The discounting values are:  {0: 0.8486194136066041, 1: 0.189990607808936, 2: 1.0593220338983051, 3: 1.736, 4: 3.3640552995391704, 5: 3.452054794520548, 6: 5.25, 7: 4.444444444444445, 8: 9.0, 9: 6.571428571428571, 10: 9.08695652173913}\n"
     ]
    }
   ],
   "source": [
    "after_count = {}\n",
    "after_count[0] = good_turing[1]/(sum(good_turing.values()))\n",
    "for i in range(1,11):\n",
    "    after_count[i] = (good_turing[i+1]*(i+1))/(good_turing[i])\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('c value used for c+1 estimation')\n",
    "plt.ylabel(' The discounting value obtained')\n",
    "plt.scatter(after_count.keys(),after_count.values())\n",
    "print(' The discounting values are: ', after_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there is a missing N(c+1) or N(c), then I am using the bigram mle here. \n",
    "### Below is the computation of probabilities using Good Turing Smoothing on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = {}\n",
    "sum_ = sum(bigram_count.values())\n",
    "for data in bigram_total:\n",
    "    if data not in bigram_count:\n",
    "        gt[data] = good_turing[1]/(sum(good_turing.values()))\n",
    "    else:\n",
    "        try:\n",
    "            gt[data] = (good_turing[bigram_count[data]]*bigram_count[data])/(good_turing[bigram_count[data]-1]*sum_)\n",
    "        except:\n",
    "            gt[data] = mle_bigram[data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity for Good Turing on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135.81947749997084\n"
     ]
    }
   ],
   "source": [
    "def good_turing_perplexity(data):\n",
    "    prob = 0\n",
    "    count = 0\n",
    "    for i in data:\n",
    "        arr = i.split()\n",
    "        for j in range(len(arr)):\n",
    "            count+=1\n",
    "            #print(math.log(add_one[(arr[j],arr[j+1])]))\n",
    "            try:\n",
    "                prob+=math.log(gt[(arr[j],arr[j+1])])\n",
    "                #print('b')\n",
    "            except:\n",
    "                continue\n",
    "    return math.exp(-1/(count)*prob)\n",
    "print (good_turing_perplexity(test_data))\n",
    "del gt\n",
    "del bigram_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clearly among both Add-One and Good Turing, Good Turing performs ~ 10 times better with a perplexity value of 135.82 over Add-One's Perplexity Value of 1324.72"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
